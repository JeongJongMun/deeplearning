{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grad of parameters in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 선언\n",
    "\n",
    "import torch\n",
    "\n",
    "class sampleClass(torch.nn.Module):\n",
    "\tdef __init__(self) -> None:\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.linear1 = torch.nn.Linear(3, 4)\n",
    "\t\tself.relu = torch.nn.ReLU()\n",
    "\t\tself.linear2 = torch.nn.Linear(4, 2)\n",
    "\t\tself.ff = torch.nn.Sequential(\n",
    "\t\tself.linear1,\n",
    "\t\tself.relu,\n",
    "\t\tself.linear2\n",
    "\t)\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.ff(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear1.weight \n",
      " tensor([[-0.0211,  0.0067, -0.0182],\n",
      "        [-0.1526, -0.0175,  0.1424],\n",
      "        [ 0.0459, -0.0131,  0.0071],\n",
      "        [ 0.2053, -0.0216, -0.2782]]) \n",
      "\n",
      "linear1.bias \n",
      " tensor([-0.0190,  0.1522,  0.0158, -0.5933]) \n",
      "\n",
      "linear2.weight \n",
      " tensor([[-0.1196, -0.2009, -0.1631, -0.4590],\n",
      "        [ 0.0122,  0.0975, -0.0093,  0.1346]]) \n",
      "\n",
      "linear2.bias \n",
      " tensor([-1.0909,  0.4438]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# named_parameters()\n",
    "\n",
    "x = torch.randn(15).reshape(5,3)\n",
    "target = torch.tensor([1., -1.0])\n",
    "model = sampleClass()\n",
    "y = model(x)\n",
    "loss = torch.square(target-y).mean(dim=-1)\n",
    "loss = loss.mean()\n",
    "loss.backward()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "\tprint(name, '\\n', param.grad, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "backward() vs autograd.grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input\n",
      " tensor([[-0.0122, -0.0181,  0.0241],\n",
      "        [ 0.0000,  0.0000,  0.0000]])\n",
      "linear1.weight \n",
      " tensor([[ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [-0.0460,  0.0031,  0.0577]])\n",
      "\n",
      "linear1.bias \n",
      " tensor([0.0000, 0.0000, 0.0000, 0.0428])\n",
      "\n",
      "linear2.weight \n",
      " tensor([[ 0.0000,  0.0000,  0.0000, -0.6578],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.7662]])\n",
      "\n",
      "linear2.bias \n",
      " tensor([-2.3440,  2.3521])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# loss.backward()\n",
    "model = sampleClass()\n",
    "model.train()\n",
    "\n",
    "target = torch.tensor([1., -1.0])\n",
    "\n",
    "input = torch.randn(2,3 , requires_grad=True)\n",
    "output = model(input)\n",
    "loss = torch.square(target-output).mean(dim=-1)\n",
    "loss = torch.sum(loss)\n",
    "loss.backward()\n",
    "\n",
    "# 모델의 Weight와 Bias와 Input들의 grad에 미분 값들이 저장된다.\n",
    "print(\"Input\\n\",input.grad)\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, '\\n', param.grad)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 0.0637,  0.0095, -0.1384],\n",
      "        [-0.0573, -0.0337,  0.0637]]),)\n",
      "Input\n",
      " None\n",
      "linear1.weight \n",
      " None \n",
      "\n",
      "linear1.bias \n",
      " None \n",
      "\n",
      "linear2.weight \n",
      " None \n",
      "\n",
      "linear2.bias \n",
      " None \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# torch.autograd.grad\n",
    "model2 = sampleClass()\n",
    "model2.train()\n",
    "\n",
    "input = torch.randn(2,3 , requires_grad=True)\n",
    "output = model2(input)\n",
    "loss = torch.square(target-output).mean(dim=-1)\n",
    "loss = torch.sum(loss)\n",
    "# input에 대한 loss의 기울기만 반환만 함 (grad에 저장 X)\n",
    "print(torch.autograd.grad(outputs=loss, inputs=input))\n",
    "\n",
    "# 모델의 Weight와 Bias와 Input들의 grad에 미분 값들이 저장되지 않는다.\n",
    "print(\"Input\\n\",input.grad)\n",
    "for name, param in model2.named_parameters():\n",
    "    print(name, '\\n', param.grad, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer.zero_grad() # Reset gradients\n",
    "# for i, (inputs, labels) in enumerate(dataloader):\n",
    "#     outputs = model(inputs)\n",
    "#     loss = criterion(outputs, labels)\n",
    "#     loss.backward() # Gradients are accumulated\n",
    "    \n",
    "#     if (i + 1) % gradient_accumulation_steps == 0:\n",
    "#         optimizer.step() # Update weights\n",
    "#         optimizer.zero_grad() # Reset gradients for the next iteration\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
