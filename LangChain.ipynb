{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경변수로 설정된 OpenAI API Key 가져오기\n",
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMs - Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_usage': {'prompt_tokens': 8, 'total_tokens': 56, 'completion_tokens': 48}, 'model_name': 'text-davinci-003'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI()\n",
    "\n",
    "#1) __call__: string in → string out\n",
    "llm(\"Tell me a joke\")\n",
    "\n",
    "#2) generate: batch calls, richer outputs\n",
    "result = llm.generate([\"Tell me a joke\", \"Tell me a poem\"])\n",
    "result.generations # Be sure to confirm “the structure of generations!!!!”\n",
    "result.llm_output # meta data of result from llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Async API - for IO Bound Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in foo\n",
      "Explicit context switch to bar\n",
      "Explicit context switch to foo again\n",
      "Implicit context switch to bar\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "async def foo():\n",
    "    print('Running in foo')\n",
    "    await asyncio.sleep(1)\n",
    "    print('Explicit context switch to foo again')\n",
    "\n",
    "async def bar():\n",
    "    print('Explicit context switch to bar')\n",
    "    await asyncio.sleep(1)\n",
    "    print('Implicit context switch to bar')\n",
    "\n",
    "async def main():\n",
    "    tasks = [foo(), bar()]\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "async def async_generate(llm):\n",
    "    resp = await llm.agenerate([\"Hello, how are you?\"])\n",
    "    print(resp.generations[0][0].text)\n",
    "    \n",
    "async def generate_concurrently():\n",
    "    llm = OpenAI(temperature=0.9)\n",
    "    tasks = [async_generate(llm) for _ in range(10)]\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "\n",
    "await main()\n",
    "# >>> await generate_concurrently()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.base import LLM, CallbackManagerForLLMRun\n",
    "from typing import Any, List, Mapping, Optional\n",
    "class CustomLLM(LLM):\n",
    "    n: int\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        if stop is not None:\n",
    "            raise ValueError(\"stop kwargs are not permitted.\")\n",
    "        return prompt[: self.n]\n",
    "    \n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {\"n\": self.n}\n",
    "    \n",
    "# >>> c_llm = CustomLLM(n=10)\n",
    "# >>> c_llm('Tell me a joke’)\n",
    "# 'Tell me a ‘\n",
    "# >>> c_llm._identifying_params\n",
    "# {'n': 10}\n",
    "# >>> c_llm._llm_type\n",
    "# 'custom'\n",
    "\n",
    "# 내가 만들 LLM을 사용하는 방법\n",
    "# LangChain에 툴이 많으니 쓰자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the chicken cross the road?\\n\\nTo get to the other side!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save your money & time\n",
    "\n",
    "import langchain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# To make the caching really obvious, lets use a slower model.\n",
    "llm = OpenAI(model_name=\"text-davinci-002\", n=2, best_of=2)\n",
    "\n",
    "# 1. In Memory Cache Method\n",
    "from langchain.cache import InMemoryCache\n",
    "langchain.llm_cache = InMemoryCache()\n",
    "\n",
    "# The first time, it is not yet in cache, so it should take longer\n",
    "# >>> llm.predict(\"Tell me a joke\")\n",
    "# 0.8s in vscode notebook\n",
    "# The second time it is, so it goes faster\n",
    "# >>> llm.predict(\"Tell me a joke\")\n",
    "# 0.0s\n",
    "\n",
    "# 2. SQLite Cache Method\n",
    "from langchain.cache import SQLiteCache\n",
    "langchain.llm_cache = SQLiteCache(database_path=\".langchain.db\")\n",
    "\n",
    "llm.predict(\"Tell me a joke\")\n",
    "# 0.7s\n",
    "# llm.predict(\"Tell me a joke\")\n",
    "# 0.0s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streaming Like ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "llm = OpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()], temperature=0)\n",
    "\n",
    "# 1) output will display like ChatGPT display\n",
    "\n",
    "resp = llm(\"Write me a song about sparkling water.\")\n",
    "# 2) generate still working with streaming mode\n",
    "llm.generate([\"Tell me a joke.\"])\n",
    "# 3) caching still working with streaming mode but immediate retrieval from a current cache\n",
    "llm(\"Write me a song about sparkling water.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracking Token Usages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 0\n",
      "\tPrompt Tokens: 0\n",
      "\tCompletion Tokens: 0\n",
      "Successful Requests: 0\n",
      "Total Cost (USD): $0.0\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "llm = OpenAI()\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    result = llm(\"Tell me a joke\")\n",
    "    print(cb)\n",
    "\n",
    "# Tokens Used: 24 Prompt Tokens: 4 Completion Tokens: 20 Successful \n",
    "# Requests: 1 Total Cost (USD): $0.00048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Model\n",
    "LLM에서 발전된게 Chat Model이니 더 많이 쓸 듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='나는 프로그래밍을 싫어해요.' additional_kwargs={} example=False\n",
      "\n",
      "나는 프로그래밍을 사랑합니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "text1 = chat([HumanMessage(content=\"Translate this sentence from English to Korean: I hate programming.\")])\n",
    "\n",
    "text2 = chat.predict(\"Translate this sentence from English to Korean: I love programming.\")\n",
    "\n",
    "print(text1)\n",
    "print()\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chat.generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[ChatGeneration(text=\"J'adore programmer.\", generation_info={'finish_reason': 'stop'}, message=AIMessage(content=\"J'adore programmer.\", additional_kwargs={}, example=False))], [ChatGeneration(text=\"J'adore l'intelligence artificielle.\", generation_info={'finish_reason': 'stop'}, message=AIMessage(content=\"J'adore l'intelligence artificielle.\", additional_kwargs={}, example=False))]], llm_output={'token_usage': {'prompt_tokens': 53, 'completion_tokens': 18, 'total_tokens': 71}, 'model_name': 'gpt-3.5-turbo'}, run=[RunInfo(run_id=UUID('0f799e5d-a6c6-4cfa-9f75-a7fa9d09cdd5')), RunInfo(run_id=UUID('eab00949-3d78-4455-b88f-152e027dc20d'))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare a list of messages\n",
    "batch_messages = [\n",
    "[\n",
    "    SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "    HumanMessage(content=\"I love programming.\")\n",
    "],\n",
    "[\n",
    "    SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "    HumanMessage(content=\"I love artificial intelligence.\")\n",
    "],\n",
    "]\n",
    "result = chat.generate(batch_messages)\n",
    "\n",
    "# result will display rich information about response messages\n",
    "result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "caching - same as LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Will ChatGPT take away software jobs?'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.cache import InMemoryCache\n",
    "langchain.llm_cache = InMemoryCache()\n",
    "#!rm .langchain.db\n",
    "# We can do the same thing with a SQLite cache\n",
    "from langchain.cache import SQLiteCache\n",
    "\n",
    "langchain.llm_cache = SQLiteCache(database_path=\".langchain.db\")\n",
    "\n",
    "chat.predict(\"Translate this sentence from Korean to English: ChatGPT가 SW일자리를 빼앗아 갈 것인가?.\")\n",
    "# 'Will ChatGPT take away software jobs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompts for chat model, streaming in chat model 생략? 슥 지나감"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Template vs Chat Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a funny joke about chickens\n"
     ]
    }
   ],
   "source": [
    "# PT\n",
    "from langchain import PromptTemplate\n",
    "# 1) get a template from .from_template()\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    template = \"Tell me a {adjective} joke about {content}\",\n",
    ")\n",
    "\n",
    "# 2) get a prompt(str) from .format()\n",
    "prompt = prompt_template.format(adjective='funny', \n",
    "content='chickens')\n",
    "\n",
    "# 3) type(prompt) → str\n",
    "# type(prompt)\n",
    "# >>> str\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='You are a helpful AI bot. Your name is Bob.' additional_kwargs={}\n",
      "content='Hello, how are you doing?' additional_kwargs={} example=False\n",
      "content=\"I'm doing well, thanks!\" additional_kwargs={} example=False\n",
      "content='What is your name?' additional_kwargs={} example=False\n"
     ]
    }
   ],
   "source": [
    "# CPT\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "# 1) get a template from .from_message()\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "    (\"human\", \"Hello, how are you doing?\"),\n",
    "    (\"ai\", \"I'm doing well, thanks!\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "])\n",
    "\n",
    "# 2) get a messages from .format_message()\n",
    "messages = template.format_messages(\n",
    "    name=\"Bob\", user_input=\"What is your name?\"\n",
    ")\n",
    "# 3) type(messages) → list\n",
    "# 4) type(element of messages) → System/Human/AI/Human Massage\n",
    "\n",
    "for mes in messages:\n",
    "    print(mes)\n",
    "# print(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "message types for from_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I absolutely adore eating tasty things!', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate, SystemMessage, HumanMessagePromptTemplate\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "            \"You are a helpful assistant that re-writes the user's text to sound more upbeat.\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "    ])\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "llm(template.format_messages(text='i dont like eating tasty things.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Template\n",
    "재미 없음 그냥 이렇게 할 수 있다~\n",
    "generate 함수는 nanoGPT에서 generate 함수 긁어다 넣음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few-shot Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (299899643.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    examples = [ {another example of the same format},{\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "examples = [ {\"another example of the same format\"},{\n",
    "\"question\": \"Who was the maternal grandfather of George Washington?\",\n",
    "\"answer\":\n",
    "\"\"\"\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: Who was the mother of George Washington?\n",
    "Intermediate answer: The mother of George Washington \n",
    "was Mary Ball Washington.\n",
    "Follow up: Who was the father of Mary Ball Washington?\n",
    "Intermediate answer: The father of Mary Ball Washington \n",
    "was Joseph Ball.\n",
    "So the final answer is: Joseph Ball\n",
    "\"\"\" },]\n",
    "\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-002\", n=2, best_of=2)\n",
    "\n",
    "example_prompt = PromptTemplate(input_variables=[\"question\", \"answer\"], template=\"Question: {question}\\n{answer}\")\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples, \n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=[\"input\"],\n",
    ")\n",
    "q_str = prompt.format(input=\"Who was the father of Mary Ball Washington?\")\n",
    "a_str = llm.predict(q_str)\n",
    "print(a_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixed (고정) 되어있는 few-shot examples for chat models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
